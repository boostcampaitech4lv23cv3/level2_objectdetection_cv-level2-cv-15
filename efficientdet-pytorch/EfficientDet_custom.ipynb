{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting effdet\n",
      "  Downloading effdet-0.3.0-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (2.0.6)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (0.8.2)\n",
      "Requirement already satisfied: timm>=0.4.12 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (0.6.11)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (1.7.1)\n",
      "Requirement already satisfied: omegaconf>=2.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (2.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from omegaconf>=2.0->effdet) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/envs/detection/lib/python3.7/site-packages (from omegaconf>=2.0->effdet) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from pycocotools>=2.0.2->effdet) (3.5.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/detection/lib/python3.7/site-packages (from pycocotools>=2.0.2->effdet) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/detection/lib/python3.7/site-packages (from timm>=0.4.12->effdet) (0.11.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/detection/lib/python3.7/site-packages (from torch>=1.4->effdet) (4.3.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from torchvision->effdet) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (3.0.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/detection/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.12->effdet) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/detection/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.12->effdet) (5.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/detection/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.12->effdet) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/detection/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.12->effdet) (4.64.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/detection/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/detection/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm>=0.4.12->effdet) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/detection/lib/python3.7/site-packages (from requests->huggingface-hub->timm>=0.4.12->effdet) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from requests->huggingface-hub->timm>=0.4.12->effdet) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/detection/lib/python3.7/site-packages (from requests->huggingface-hub->timm>=0.4.12->effdet) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/detection/lib/python3.7/site-packages (from requests->huggingface-hub->timm>=0.4.12->effdet) (2.1.1)\n",
      "Installing collected packages: effdet\n",
      "Successfully installed effdet-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install effdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ttach\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: ttach\n",
      "Successfully installed ttach-0.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ttach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cTHDyODENsgr"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈 import\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SENfg9eZNsgu"
   },
   "outputs": [],
   "source": [
    "# CustomDataset class 선언\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # coco annotation 불러오기 (by. coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(), #이미지의 주소\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(), # 카테고리 리스트\n",
    "            \"annotations\": None # annotation정보\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        annotations_train = annotation\n",
    "        with open(annotations_train, 'r') as f:\n",
    "            train_json = json.loads(f.read())\n",
    "            train_images = train_json['images']\n",
    "        self.id = []\n",
    "        for img in train_images:\n",
    "            self.id.append(img['id'])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        index = self.id[index]\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # boxes (x, y, w, h)\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # (x,y,w,h)의 coco format에서 (x_min, y_min, x_max, y_max)로 변경\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] \n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        # for i in boxes:\n",
    "        #     i[2] = max(1024.0, i[0]+i[2])\n",
    "        #     i[3] = max(1024.0, i[1]+i[3])\n",
    "        \n",
    "        \n",
    "        # box별 label\n",
    "        labels = np.array([x['category_id']+1 for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            while True:\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    target['labels'] = torch.tensor(sample['labels'])\n",
    "                    break\n",
    "            \n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W8FQbYrjNsgx"
   },
   "outputs": [],
   "source": [
    "# # Albumentation을 이용, augmentation 선언\n",
    "# def get_train_transform():\n",
    "#     return A.Compose([\n",
    "#         A.Resize(1024, 1024),\n",
    "#         A.OneOf([\n",
    "#             A.Flip(p=0.5),\n",
    "#             A.RandomRotate90(p=1.0)\n",
    "#         ]),\n",
    "#         A.OneOf([\n",
    "#             A.Blur(p=1.0),\n",
    "#             A.MedianBlur(blur_limit=3, p=.10),\n",
    "#             A.Sharpen(p=1.0)\n",
    "#         ]),\n",
    "#         A.RandomBrightnessContrast(p=1.0),\n",
    "#         ToTensorV2(p=1.0)\n",
    "#     ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "# def get_valid_transform():\n",
    "#     return A.Compose([\n",
    "#         A.Resize(1024, 1024),\n",
    "#         ToTensorV2(p=1.0)\n",
    "#     ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        A.OneOf([\n",
    "                    A.Flip(p=1.0),\n",
    "                    A.RandomRotate90(p=1.0)\n",
    "                ], p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.15, p=0.5),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.OneOf([\n",
    "            A.Blur(p=1.0),\n",
    "            A.GaussianBlur(p=1.0),\n",
    "            A.MedianBlur(blur_limit=5, p=1.0),\n",
    "            A.MotionBlur(p=1.0)\n",
    "        ], p=0.1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Kc0uStKpNsgy"
   },
   "outputs": [],
   "source": [
    "# loss 추적\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kGjDutKiNsgy"
   },
   "outputs": [],
   "source": [
    "# Effdet config\n",
    "# https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/config/model_config.py\n",
    "\n",
    "# Effdet config를 통해 모델 불러오기\n",
    "def get_net(checkpoint_path=None):\n",
    "    \n",
    "    config = get_efficientdet_config('tf_efficientdet_d4_ap')\n",
    "    config.num_classes = 10\n",
    "    config.image_size = (1024,1024)\n",
    "    \n",
    "    config.soft_nms = False\n",
    "    config.max_det_per_image = 30\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    return DetBenchTrain(net)\n",
    "    \n",
    "# train function\n",
    "def train_fn(num_epochs, train_data_loader, optimizer, model, device, clip=35):\n",
    "    loss_hist = Averager()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_hist.reset()\n",
    "        \n",
    "        for images, targets, image_ids in tqdm(train_data_loader):\n",
    "            \n",
    "                images = torch.stack(images) # bs, ch, w, h - 16, 3, 512, 512\n",
    "                images = images.to(device).float()\n",
    "                boxes = [target['boxes'].to(device).float() for target in targets]\n",
    "                labels = [target['labels'].to(device).float() for target in targets]\n",
    "                target = {\"bbox\": boxes, \"cls\": labels}\n",
    "\n",
    "                # calculate loss\n",
    "                loss, cls_loss, box_loss = model(images, target).values()\n",
    "                loss_value = loss.detach().item()\n",
    "                \n",
    "                loss_hist.send(loss_value)\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # grad clip\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n",
    "        torch.save(model.state_dict(), f'epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OS1UR7dqNsgz"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    annotation = '../../dataset/train.json'\n",
    "    data_dir = '../../dataset'\n",
    "    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    model = get_net()\n",
    "    model.to(device)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    optimizer = torch.optim.Adam(params, lr=0.0005, weight_decay=0.0005)\n",
    "    \n",
    "    num_epochs = 50\n",
    "\n",
    "    loss = train_fn(num_epochs, train_data_loader, optimizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "h5kBcuWbNsg0",
    "outputId": "63068363-da84-466a-96d3-5a7cf28a7aad",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 154/1221 [02:15<15:39,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7562/217905245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7562/2329527831.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7562/559426316.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(num_epochs, train_data_loader, optimizer, model, device, clip)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# grad clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 29 17:49:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   43C    P0    39W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientDet_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b94c6de4bce9a87a354a5fa9998691adc0532adddb9d4140f5ba941d00b01fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
